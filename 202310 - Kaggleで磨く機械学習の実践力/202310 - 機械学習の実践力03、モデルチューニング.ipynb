{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# モデルチューニング\n",
    "特徴量選択・ラッパー法による精度向上に加え、LGBMのモデルを最適化することで精度向上を行う。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "import gc\n",
    "\n",
    "# 分布確認に使う\n",
    "# import pandas_profiling as pdp\n",
    "# 可視化\n",
    "import matplotlib.pyplot as plt\n",
    "# 前処理、特徴量作成 - sklearnを使う\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder\n",
    "# モデリング・精度と評価指標\n",
    "from sklearn.model_selection import train_test_split, KFold, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix\n",
    "#LGBM\n",
    "import lightgbm as lgb\n",
    "\n",
    "#どうでもいい警告は無視\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# NOTE matplotでの日本語文字化けを解消\n",
    "#pip install japanize-matplotlib\n",
    "import japanize_matplotlib\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# データ確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"titanic_datasets/train.csv\")\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"データ形状：\")\n",
    "print(df_train.shape)\n",
    "\n",
    "print(\"データ数：\")\n",
    "print(len(df_train))\n",
    "\n",
    "print(\"データのコラム数\")\n",
    "print(len(df_train.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"データ型一覧\")\n",
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 量的変数の要約統計\n",
    "df_train.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#質的変数の度数\n",
    "df_train.describe(exclude=\"number\").T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# データ前処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#性別(sex)→0,1\n",
    "# 古典的な男女の質的変数など、ダミー化した情報が1つで十分な場合は多重共線性対策で1つのみ情報を残す\n",
    "df_train = pd.get_dummies(df_train, columns = [\"Sex\"], drop_first=True)\n",
    "\n",
    "#出港地(Embarked)→0,1,2\n",
    "#df_train['Embarked'].fillna(('S'), inplace=True)\n",
    "df_train = pd.get_dummies(df_train, columns = [\"Embarked\"])\n",
    "\n",
    "#運賃(Fare)→欠損値は平均値にする\n",
    "df_train['Fare'].fillna(np.mean(df_train['Fare']), inplace=True)\n",
    "#運賃(Fare)のヒストグラムを描写。ポアソン分布となる説明変数はビニングすると精度が向上する事が分かっている\n",
    "#df_train['Fare'].hist(bins=50)\n",
    "\n",
    "# NOTE 境界値を指定したビニング\n",
    "bins_Fare = [0,50,100,200,300,1000]\n",
    "# T_Bil列を分割し、0始まりの連番でラベル化した結果をX_cutに格納する\n",
    "X_cut, bin_indice = pd.cut(df_train[\"Fare\"], bins=bins_Fare, retbins=True, labels=False)\n",
    "# bin分割した結果をダミー変数化 (prefix=X_Cut.nameは、列名の接頭語を指定している)\n",
    "X_dummies = pd.get_dummies(X_cut, prefix=X_cut.name)\n",
    "# 元の説明変数のデータフレーム(X)と、ダミー変数化した結果(X_dummies)を横連結\n",
    "df_train = pd.concat([df_train, X_dummies], axis=1)\n",
    "\n",
    "#年齢(Age)→欠損値は平均値・標準偏差を使って正規乱数で埋め合わせる\n",
    "age_avg = df_train['Age'].mean()\n",
    "age_std = df_train['Age'].std()\n",
    "df_train['Age'].fillna(np.random.randint(age_avg - age_std, age_avg + age_std), inplace=True)\n",
    "\n",
    "# 特徴量：Familysizeを作成(Parch, Sibspを足した値)\n",
    "df_train['FamilySize'] = df_train['Parch'] + df_train['SibSp'] + 1\n",
    "df_train['FamilySize_bin'] = 'big'\n",
    "df_train.loc[df_train['FamilySize']==1,'FamilySize_bin'] = 'alone'\n",
    "df_train.loc[(df_train['FamilySize']>=2) & (df_train[\"FamilySize\"]<=4),'FamilySize_bin'] = 'small'\n",
    "df_train.loc[(df_train['FamilySize']>=5) & (df_train[\"FamilySize\"]<=7),'FamilySize_bin'] = 'mediam'\n",
    "\n",
    "# 特徴量：TicketFreq - Ticket頻度をチケットを表す量的変数とする\n",
    "df_train.loc[:, 'TicketFreq'] = df_train.groupby(['Ticket'])['PassengerId'].transform('count')\n",
    "\n",
    "# 特徴量：HonorificをNameから抽出\n",
    "df_train['honorific'] = df_train['Name'].map(lambda x: x.split(', ')[1].split('. ')[0])\n",
    "df_train['honorific'].replace(['Col','Dr', 'Rev'], 'Rare',inplace=True) #少数派の敬称を統合\n",
    "df_train['honorific'].replace('Mlle', 'Miss',inplace=True) #Missに統合\n",
    "df_train['honorific'].replace('Ms', 'Miss',inplace=True) #Missに統合\n",
    "\n",
    "# 特徴量：Cabin_ini - Cabinの頭文字を抽出\n",
    "df_train['Cabin_ini'] = df_train['Cabin'].map(lambda x:str(x)[0])\n",
    "df_train['Cabin_ini'].replace(['G','T'], 'Rare',inplace=True)\n",
    "\n",
    "# FamilySize_bin, honorific, Cabin_iniをすべてダミー化する\n",
    "df_train = pd.get_dummies(df_train, columns = [\"FamilySize_bin\", \"honorific\", \"Cabin_ini\"])\n",
    "\n",
    "print(\"コラム名:\")\n",
    "print(df_train.columns)\n",
    "\n",
    "print(\"コラム数:\")\n",
    "print(len(df_train.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 特徴量選択"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_col = 'Survived'\n",
    "drop_col = ['PassengerId','Survived', 'Name', 'Fare', 'Ticket', 'Cabin', 'Parch', 'FamilySize', 'SibSp']\n",
    "\n",
    "y = df_train[target_col]\n",
    "X = df_train.drop(drop_col , axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ラッパー法(GBDT)で説明変数を削減"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "# estimatorとしてGBDTを使用。特徴量を20個選択\n",
    "selector = RFE(GradientBoostingRegressor(n_estimators=100, random_state=10), n_features_to_select=20)\n",
    "selector.fit(X, y)\n",
    "mask = selector.get_support()\n",
    "#print(X.feature_names)\n",
    "print(mask)\n",
    "\n",
    "# 選択した特徴量の列のみ取得\n",
    "X_selected = selector.transform(X)\n",
    "print(\"X.shape={}, X_selected.shape={}\".format(X.shape, X_selected.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list = []\n",
    "not_selected = []\n",
    "columns = X.columns\n",
    "\n",
    "for i in range(0, len(mask)):\n",
    "    \n",
    "    value = mask[i]\n",
    "    \n",
    "    if (value == True):\n",
    "        list.append(columns[i])\n",
    "    else:\n",
    "        not_selected.append(columns[i])\n",
    "\n",
    "print(\"選択された20の特徴量：\")\n",
    "print(list)\n",
    "\n",
    "print(\"選択されなかった特徴量：\")\n",
    "print(not_selected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# モデル作成・学習\n",
    "今回は学習段階でのハイパーパラメータを最適化し,汎用精度の向上を目指します。\n",
    "モデルの最適化には、大きく2つの手法がある\n",
    "- 手動チューニング - カンを頼りにパラメータを変える\n",
    "- 自動チューニング - Optuna, scikit-optimizeなどを利用して自動で分類精度の高いパラメータを探索する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, id_train = X[list], df_train[[\"Survived\"]], df_train[[\"PassengerId\"]]\n",
    "\n",
    "# データ形状が問題ないか判定\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(id_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "→true・falseを0・1に変換、scikit-learnの別のアルゴリズムを活用してアンサンブル学習をできるようにする。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ホールドアウト検証 - 学習用・テスト用に分割\n",
    "X_tr, X_va, y_tr, y_va = train_test_split(X_train, y_train, test_size=0.2, shuffle=True, stratify=y_train, random_state=123)\n",
    "\n",
    "print(\"学習用・訓練用データの形状：\")\n",
    "print(X_tr.shape)\n",
    "print(y_tr.shape)\n",
    "print(X_va.shape)\n",
    "print(y_va.shape)\n",
    "    \n",
    "# データのラベルに偏りがないか表示\n",
    "y_train_mean = y_train[\"Survived\"].mean()\n",
    "y_tr_mean = y_tr[\"Survived\"].mean()\n",
    "y_va_mean = y_va[\"Survived\"].mean()\n",
    "    \n",
    "print(\"全体の生存者割合：\")\n",
    "print(y_train_mean)\n",
    "print(\"学習用データの生存者割合：\")\n",
    "print(y_tr_mean)\n",
    "print(\"検証用データの生存者割合：\")\n",
    "print(y_va_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LGBMのパラメータ(学習しないパラメータのみこの段階で定義する)\n",
    "params_base = {\"boosting_type\":\"gbdt\",\n",
    "          \"objective\":\"binary\",\n",
    "          \"metric\":\"auc\",\n",
    "          \"learning_rate\":0.02,\n",
    "          \"num_leaves\":16,\n",
    "          \"n_estimators\":100000,\n",
    "          \"bagging_freq:\":1,\n",
    "          \"seed\": 123,\n",
    "          }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 最適パラメータの探索\n",
    "optunaでベイズ最適化によるチューニングを行う。  scikit-learnならGridSearchCVでグリッドサーチをするのもあり。\n",
    "基本探索するべきパラメータが5以上ある場合はベイズ最適によるチューニングのほうが効率的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# チューニングで探索する最適パラメータ\n",
    "def objective(trial):\n",
    "    params_tuning = {\n",
    "        \"num_leaves\": trial.suggest_int(\"num_leaves\", 8, 256),\n",
    "        \"min_data_in_leaf\":trial.suggest_int(\"min_data_in_leaf\", 5,200),\n",
    "        \"min_sum_hessian_in_leaf\":trial.suggest_float(\"min_sum_hessian_in_leaf\", 0.00001, 0.01, log = True),\n",
    "        \"feature_fraction\":trial.suggest_float(\"feature_fraction\", 0.5, 1.0),\n",
    "        \"bagging_fraction\":trial.suggest_float(\"bagging_fraction\", 0.5, 1.0),\n",
    "        \"lambda_l1\":trial.suggest_float(\"lambda_l1\", 0.01, 10.0, log = True),\n",
    "        \"lambda_l2\":trial.suggest_float(\"lambda_l2\", 0.01, 10.0, log = True)\n",
    "    }\n",
    "    \n",
    "    #tuningにbaseの値を加える\n",
    "    params_tuning.update(params_base)\n",
    "    \n",
    "    #モデル学習(ベイズ最適化)\n",
    "    list_metrics=[]\n",
    "    \n",
    "    # ホールドアウト検証 - 学習用・テスト用の分割を1通り決める\n",
    "    X_tr, X_va, y_tr, y_va = train_test_split(X_train, y_train, test_size=0.2, shuffle=True, stratify=y_train, random_state=123)\n",
    "\n",
    "        \n",
    "    model = lgb.LGBMClassifier(**params_tuning)\n",
    "        \n",
    "    model.fit(X_tr, y_tr, eval_set=[(X_tr, y_tr),(X_va, y_va)], \n",
    "                    #early_stopping_rounds=100, \n",
    "                    #verbose=0\n",
    "                    )\n",
    "    y_va_pred = model.predict_proba(X_va)[:, 1]\n",
    "    metric_va = accuracy_score(y_va, np.where(y_va_pred >= 0.5, 1, 0))\n",
    "    \n",
    "    return metric_va"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 探索を実行\n",
    "sampler = optuna.samplers.TPESampler(seed=123)\n",
    "study = optuna.create_study(sampler=sampler, direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 探索で得られた結果を確認\n",
    "trial = study.best_trial\n",
    "print(\"最も高い精度：\")\n",
    "print(trial.value)\n",
    "\n",
    "print(\"最も高い精度となるパラメータ：\")\n",
    "print(trial.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#param_best = trial.params\n",
    "param_best = {'num_leaves': 219, 'min_data_in_leaf': 146, 'min_sum_hessian_in_leaf': 0.0006808799287054756, 'feature_fraction': 0.8612216912851107, 'bagging_fraction': 0.6614794569265892, 'lambda_l1': 0.1217211275847546, 'lambda_l2': 0.048393796358465295}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 元のパラメータに最高精度が得られたパラメータを足す\n",
    "#param_best = trial.params\n",
    "params_best = {'num_leaves': 219, 'min_data_in_leaf': 146, 'min_sum_hessian_in_leaf': 0.0006808799287054756, 'feature_fraction': 0.8612216912851107, 'bagging_fraction': 0.6614794569265892, 'lambda_l1': 0.1217211275847546, 'lambda_l2': 0.048393796358465295}\n",
    "params_best.update(params_base)\n",
    "display(params_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LGBMのモデルを学習開始\n",
    "model = lgb.LGBMClassifier(**params_best)\n",
    "model.fit(X_tr, y_tr, eval_set=[(X_tr, y_tr),(X_va, y_va)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AUC値に加え精度を算出する\n",
    "y_tr_pred = model.predict(X_tr)\n",
    "y_va_pred = model.predict(X_va)\n",
    "\n",
    "metric_tr = accuracy_score(y_tr, y_tr_pred)\n",
    "metric_va = accuracy_score(y_va, y_va_pred)\n",
    "print(\"モデル精度:\")\n",
    "print(\"学習精度\")\n",
    "print(metric_tr)\n",
    "print(\"検証精度\")\n",
    "print(metric_va)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lazypredict\n",
    "from lazypredict.Supervised import LazyClassifier\n",
    "\n",
    "reg = LazyClassifier(ignore_warnings=True, random_state=1121, verbose=False,predictions=True)\n",
    "models, predictions = reg.fit(X_tr, X_va, y_tr, y_va) \n",
    "\n",
    "print(\"モデルの精度・評価指標：\")\n",
    "display(models)\n",
    "print(\"テストデータの予測値：\")\n",
    "display(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=model.predict(X_va)\n",
    "y_pred[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 精度検証 - 混合行列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 混合行列 - ラベルの正誤の分類数をまとめる\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "cm = confusion_matrix(y_pred, y_va)\n",
    "print(\"混合行列：\")\n",
    "print(cm)\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"0\",\"1\"])\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "\"\"\"\n",
    "適合率は予測されたラベルのうち、どれだけが正解だったか\n",
    "再現率は正解のラベルのうち、どれだけそのラベルを予測できたか\n",
    "\n",
    "一般的に誤検出を少なくしたい場合には適合率。取りこぼしを少なくしたい場合には再現率を重視します。\n",
    "\n",
    "医療等に機械学習を活用する等「取りこぼしが多いと命に関わるような重大な問題に繋がる」といったような場合には、再現率を評価指標に使用するのが適切です。\n",
    "\"\"\"\n",
    "\n",
    "#適合率 - TP+TN / (TP+FP+TN+FN) - 単純な正答率\n",
    "print(\"正答率\")\n",
    "print(accuracy_score(y_pred, y_va))\n",
    "\n",
    "#適合率 - TP / (TP+FP) - 予測されたラベルに対する実際に正解となるサンプル\n",
    "print(\"適合率\")\n",
    "print(precision_score(y_pred, y_va))\n",
    "\n",
    "#再現率 - TP / (TP+FN) - 本来正しく予測した場合に対し実際に正解となるサンプル\n",
    "print(\"再現率\")\n",
    "print(recall_score(y_pred, y_va))\n",
    "\n",
    "print(\"F値\")\n",
    "print(f1_score(y_pred, y_va))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LazyPredict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lazypredict\n",
    "from lazypredict.Supervised import LazyClassifier\n",
    "\n",
    "reg = LazyClassifier(ignore_warnings=True, random_state=1121, verbose=False,predictions=True)\n",
    "models, predictions = reg.fit(X_tr, X_va, y_tr, y_va) \n",
    "\n",
    "print(\"モデルの精度・評価指標：\")\n",
    "display(models)\n",
    "print(\"テストデータの予測値：\")\n",
    "display(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 提出用データを作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv(\"titanic_datasets/test.csv\")\n",
    "df_test.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#性別(sex)→0,1\n",
    "# 古典的な男女の質的変数など、ダミー化した情報が1つで十分な場合は多重共線性対策で1つのみ情報を残す\n",
    "df_test = pd.get_dummies(df_test, columns = [\"Sex\"], drop_first=True)\n",
    "\n",
    "#出港地(Embarked)→0,1,2、不明の値は仮にS=0にする\n",
    "df_test['Embarked'].fillna(('S'), inplace=True)\n",
    "df_test = pd.get_dummies(df_test, columns = [\"Embarked\"])\n",
    "\n",
    "#運賃(Fare)→欠損値は平均値にする\n",
    "df_test['Fare'].fillna(np.mean(df_test['Fare']), inplace=True)\n",
    "#運賃(Fare)のヒストグラムを描写。ポアソン分布となる説明変数はビニングすると精度が向上する事が分かっている\n",
    "df_test['Fare'].hist(bins=50)\n",
    "\n",
    "# NOTE 境界値を指定したビニング\n",
    "bins_Fare = [0,50,100,200,300,1000]\n",
    "# T_Bil列を分割し、0始まりの連番でラベル化した結果をX_cutに格納する\n",
    "X_cut, bin_indice = pd.cut(df_test[\"Fare\"], bins=bins_Fare, retbins=True, labels=False)\n",
    "# bin分割した結果をダミー変数化 (prefix=X_Cut.nameは、列名の接頭語を指定している)\n",
    "X_dummies = pd.get_dummies(X_cut, prefix=X_cut.name)\n",
    "# 元の説明変数のデータフレーム(X)と、ダミー変数化した結果(X_dummies)を横連結\n",
    "df_test = pd.concat([df_test, X_dummies], axis=1)\n",
    "\n",
    "#年齢(Age)→欠損値は平均値・標準偏差を使って正規乱数で埋め合わせる\n",
    "age_avg = df_test['Age'].mean()\n",
    "age_std = df_test['Age'].std()\n",
    "df_test['Age'].fillna(np.random.randint(age_avg - age_std, age_avg + age_std), inplace=True)\n",
    "\n",
    "# 特徴量：Familysizeを作成(Parch, Sibspを足した値)\n",
    "df_test['FamilySize'] = df_test['Parch'] + df_test['SibSp'] + 1 #ALLデータ\n",
    "# FamilySizeを離散化\n",
    "df_test['FamilySize_bin'] = 'big'\n",
    "df_test.loc[df_test['FamilySize']==1,'FamilySize_bin'] = 'alone'\n",
    "df_test.loc[(df_test['FamilySize']>=2) & (df_test[\"FamilySize\"]<=4),'FamilySize_bin'] = 'small'\n",
    "df_test.loc[(df_test['FamilySize']>=5) & (df_test[\"FamilySize\"]<=7),'FamilySize_bin'] = 'mediam'\n",
    "\n",
    "# 特徴量：TicketFreq - Ticket頻度をチケットを表す量的変数とする\n",
    "df_test.loc[:, 'TicketFreq'] = df_test.groupby(['Ticket'])['PassengerId'].transform('count')\n",
    "\n",
    "# 特徴量：HonorificをNameから抽出\n",
    "df_test['honorific'] = df_test['Name'].map(lambda x: x.split(', ')[1].split('. ')[0])\n",
    "df_test['honorific'].replace(['Col','Dr', 'Rev'], 'Rare',inplace=True) #少数派の敬称を統合\n",
    "df_test['honorific'].replace('Mlle', 'Miss',inplace=True) #Missに統合\n",
    "df_test['honorific'].replace('Ms', 'Miss',inplace=True) #Missに統合\n",
    "\n",
    "# 特徴量：Cabin_ini - Cabinの頭文字を抽出\n",
    "df_test['Cabin_ini'] = df_test['Cabin'].map(lambda x:str(x)[0])\n",
    "df_test['Cabin_ini'].replace(['G','T'], 'Rare',inplace=True) #少数派のCabin_iniを統合\n",
    "\n",
    "df_test = pd.get_dummies(df_test, columns = [\"FamilySize_bin\", \"honorific\", \"Cabin_ini\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_feature = df_test[list]\n",
    "test_feature.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(test_feature)\n",
    "\n",
    "# NOTE 予測結果を1次元ベクトルにする\n",
    "y_pred = np.squeeze(y_pred)\n",
    "y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred[(y_pred >= 0.5)] = 1\n",
    "y_pred[(y_pred < 0.5)] = 0\n",
    "y_pred = np.array(y_pred, dtype='int')\n",
    "\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = pd.DataFrame({'PassengerId': df_test.PassengerId, 'Survived': y_pred})\n",
    "output.to_csv('submission_LGBM_bayes_optimized_.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
